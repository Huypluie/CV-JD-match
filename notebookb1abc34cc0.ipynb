{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13297086,"sourceType":"datasetVersion","datasetId":8323132}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PyMuPDF\n!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:22:10.511775Z","iopub.execute_input":"2025-10-08T05:22:10.512361Z","iopub.status.idle":"2025-10-08T05:22:19.653803Z","shell.execute_reply.started":"2025-10-08T05:22:10.512337Z","shell.execute_reply":"2025-10-08T05:22:19.653002Z"}},"outputs":[{"name":"stdout","text":"Collecting PyMuPDF\n  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.26.4\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\nRequirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (46.0.1)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\nDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20250506 pdfplumber-0.11.7\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# PARSER PDF LAYOUT NHIá»€U Cá»˜T - PHÃT HIá»†N VÃ€ Sáº®P Xáº¾P ÄÃšNG THá»¨ Tá»°\n\n\"\"\"\nGiáº£i phÃ¡p cho PDF cÃ³ layout 2-3 cá»™t:\n1. PhÃ¡t hiá»‡n vá»‹ trÃ­ cÃ¡c text blocks\n2. PhÃ¢n loáº¡i thÃ nh cÃ¡c cá»™t\n3. Äá»c theo thá»© tá»±: cá»™t trÃ¡i â†’ cá»™t pháº£i â†’ xuá»‘ng dÃ²ng\n\"\"\"\n\nimport fitz  # PyMuPDF\nimport re\nfrom collections import defaultdict\n\n# ============================================================================\n# GIáº¢I PHÃP 1: PYMUPDF Vá»šI PHÃT HIá»†N Cá»˜T Tá»° Äá»˜NG\n# ============================================================================\n\ndef extract_pdf_with_column_detection(pdf_path, column_count=2):\n    \"\"\"\n    TrÃ­ch xuáº¥t PDF vá»›i phÃ¡t hiá»‡n cá»™t tá»± Ä‘á»™ng\n    \n    Args:\n        pdf_path: ÄÆ°á»ng dáº«n file PDF\n        column_count: Sá»‘ cá»™t (2 hoáº·c 3), hoáº·c 'auto' Ä‘á»ƒ tá»± Ä‘á»™ng phÃ¡t hiá»‡n\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    full_text = []\n    \n    for page_num, page in enumerate(doc, 1):\n        # Láº¥y kÃ­ch thÆ°á»›c trang\n        page_width = page.rect.width\n        page_height = page.rect.height\n        \n        # Láº¥y text blocks vá»›i vá»‹ trÃ­\n        blocks = page.get_text(\"dict\")[\"blocks\"]\n        \n        # PhÃ¢n loáº¡i blocks theo cá»™t\n        text_blocks = []\n        for block in blocks:\n            if \"lines\" in block:  # Text block\n                x0, y0, x1, y1 = block[\"bbox\"]\n                block_text = \"\"\n                for line in block[\"lines\"]:\n                    for span in line[\"spans\"]:\n                        block_text += span[\"text\"] + \" \"\n                \n                if block_text.strip():\n                    text_blocks.append({\n                        \"text\": block_text.strip(),\n                        \"x0\": x0,\n                        \"y0\": y0,\n                        \"x1\": x1,\n                        \"y1\": y1,\n                        \"x_center\": (x0 + x1) / 2,\n                        \"y_center\": (y0 + y1) / 2\n                    })\n        \n        # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ cá»™t náº¿u column_count == 'auto'\n        if column_count == 'auto':\n            column_count = detect_column_count(text_blocks, page_width)\n        \n        # Sáº¯p xáº¿p blocks theo cá»™t vÃ  vá»‹ trÃ­\n        if column_count == 2:\n            sorted_text = sort_two_column_layout(text_blocks, page_width)\n        elif column_count == 3:\n            sorted_text = sort_three_column_layout(text_blocks, page_width)\n        else:\n            # Single column - chá»‰ cáº§n sort theo y\n            text_blocks.sort(key=lambda b: b[\"y0\"])\n            sorted_text = \"\\n\".join([b[\"text\"] for b in text_blocks])\n        \n        full_text.append(f\"--- Trang {page_num} ---\\n{sorted_text}\")\n    \n    doc.close()\n    return \"\\n\\n\".join(full_text)\n\n\ndef detect_column_count(blocks, page_width):\n    \"\"\"\n    Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ cá»™t dá»±a trÃªn phÃ¢n bá»‘ x cá»§a cÃ¡c blocks\n    \"\"\"\n    if not blocks:\n        return 1\n    \n    x_centers = [b[\"x_center\"] for b in blocks]\n    \n    # Chia page thÃ nh 2 ná»­a\n    left_blocks = sum(1 for x in x_centers if x < page_width / 2)\n    right_blocks = sum(1 for x in x_centers if x >= page_width / 2)\n    \n    # Náº¿u cáº£ 2 ná»­a Ä‘á»u cÃ³ nhiá»u blocks â†’ 2 cá»™t\n    if left_blocks > 3 and right_blocks > 3:\n        return 2\n    \n    return 1\n\n\ndef sort_two_column_layout(blocks, page_width):\n    \"\"\"\n    Sáº¯p xáº¿p text blocks cho layout 2 cá»™t\n    Äá»c theo thá»© tá»±: trÃ¡iâ†’pháº£i, trÃªnâ†’dÆ°á»›i\n    \"\"\"\n    # Chia thÃ nh 2 cá»™t dá»±a trÃªn x_center\n    left_column = []\n    right_column = []\n    \n    mid_x = page_width / 2\n    \n    for block in blocks:\n        if block[\"x_center\"] < mid_x:\n            left_column.append(block)\n        else:\n            right_column.append(block)\n    \n    # Sáº¯p xáº¿p má»—i cá»™t theo vá»‹ trÃ­ y (tá»« trÃªn xuá»‘ng)\n    left_column.sort(key=lambda b: b[\"y0\"])\n    right_column.sort(key=lambda b: b[\"y0\"])\n    \n    # GhÃ©p text theo thá»© tá»±: cá»™t trÃ¡i trÆ°á»›c, cá»™t pháº£i sau\n    result = []\n    \n    # Cá»™t trÃ¡i\n    if left_column:\n        result.append(\"=== Cá»˜T TRÃI ===\")\n        for block in left_column:\n            result.append(block[\"text\"])\n    \n    # Cá»™t pháº£i\n    if right_column:\n        result.append(\"\\n=== Cá»˜T PHáº¢I ===\")\n        for block in right_column:\n            result.append(block[\"text\"])\n    \n    return \"\\n\".join(result)\n\n\ndef sort_three_column_layout(blocks, page_width):\n    \"\"\"\n    Sáº¯p xáº¿p text blocks cho layout 3 cá»™t\n    \"\"\"\n    left_column = []\n    middle_column = []\n    right_column = []\n    \n    third_x = page_width / 3\n    \n    for block in blocks:\n        x = block[\"x_center\"]\n        if x < third_x:\n            left_column.append(block)\n        elif x < 2 * third_x:\n            middle_column.append(block)\n        else:\n            right_column.append(block)\n    \n    # Sáº¯p xáº¿p theo y\n    left_column.sort(key=lambda b: b[\"y0\"])\n    middle_column.sort(key=lambda b: b[\"y0\"])\n    right_column.sort(key=lambda b: b[\"y0\"])\n    \n    result = []\n    for col_name, column in [(\"TRÃI\", left_column), (\"GIá»®A\", middle_column), (\"PHáº¢I\", right_column)]:\n        if column:\n            result.append(f\"\\n=== Cá»˜T {col_name} ===\")\n            result.extend([b[\"text\"] for b in column])\n    \n    return \"\\n\".join(result)\n\n\n# ============================================================================\n# GIáº¢I PHÃP 2: Sáº®P Xáº¾P THÃ”NG MINH Há»šN - THEO VÃ™NG SEMANTIC\n# ============================================================================\n\ndef extract_pdf_smart_layout(pdf_path):\n    \"\"\"\n    TrÃ­ch xuáº¥t PDF vá»›i phÃ¡t hiá»‡n layout thÃ´ng minh\n    Tá»± Ä‘á»™ng nháº­n biáº¿t: header, sidebar, main content\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    full_text = []\n    \n    for page_num, page in enumerate(doc, 1):\n        page_width = page.rect.width\n        page_height = page.rect.height\n        \n        blocks = page.get_text(\"dict\")[\"blocks\"]\n        \n        # PhÃ¢n loáº¡i blocks\n        text_blocks = []\n        for block in blocks:\n            if \"lines\" in block:\n                x0, y0, x1, y1 = block[\"bbox\"]\n                block_text = \"\"\n                font_sizes = []\n                \n                for line in block[\"lines\"]:\n                    for span in line[\"spans\"]:\n                        block_text += span[\"text\"] + \" \"\n                        font_sizes.append(span[\"size\"])\n                \n                if block_text.strip():\n                    text_blocks.append({\n                        \"text\": block_text.strip(),\n                        \"x0\": x0,\n                        \"y0\": y0,\n                        \"x1\": x1,\n                        \"y1\": y1,\n                        \"width\": x1 - x0,\n                        \"height\": y1 - y0,\n                        \"avg_font_size\": sum(font_sizes) / len(font_sizes) if font_sizes else 0\n                    })\n        \n        # PhÃ¢n loáº¡i semantic\n        header_blocks = []  # y < 15% page\n        left_sidebar = []   # x < 30% page, y > 15%\n        main_content = []   # x > 30% page, y > 15%\n        \n        for block in text_blocks:\n            # Header (pháº§n trÃªn cÃ¹ng)\n            if block[\"y0\"] < page_height * 0.15:\n                header_blocks.append(block)\n            # Left sidebar (cá»™t trÃ¡i)\n            elif block[\"x0\"] < page_width * 0.35:\n                left_sidebar.append(block)\n            # Main content (cá»™t pháº£i)\n            else:\n                main_content.append(block)\n        \n        # Sáº¯p xáº¿p má»—i vÃ¹ng\n        header_blocks.sort(key=lambda b: b[\"y0\"])\n        left_sidebar.sort(key=lambda b: b[\"y0\"])\n        main_content.sort(key=lambda b: b[\"y0\"])\n        \n        # GhÃ©p láº¡i theo thá»© tá»± logic\n        result = []\n        \n        # Header (tÃªn, title)\n        if header_blocks:\n            result.append(\"=== HEADER ===\")\n            for block in header_blocks:\n                result.append(block[\"text\"])\n        \n        # Main content trÆ°á»›c (thÆ°á»ng lÃ  pháº§n quan trá»ng)\n        if main_content:\n            result.append(\"\\n=== Ná»˜I DUNG CHÃNH ===\")\n            for block in main_content:\n                result.append(block[\"text\"])\n        \n        # Left sidebar sau (contact info, skills)\n        if left_sidebar:\n            result.append(\"\\n=== THÃ”NG TIN BÃŠN ===\")\n            for block in left_sidebar:\n                result.append(block[\"text\"])\n        \n        full_text.append(f\"--- Trang {page_num} ---\\n\" + \"\\n\".join(result))\n    \n    doc.close()\n    return \"\\n\\n\".join(full_text)\n\n\n# ============================================================================\n# GIáº¢I PHÃP 3: Sá»¬ Dá»¤NG PDFPLUMBER Vá»šI CUSTOM LAYOUT\n# ============================================================================\n\nimport pdfplumber\n\ndef extract_pdf_pdfplumber_columns(pdf_path, column_count=2):\n    \"\"\"\n    Sá»­ dá»¥ng pdfplumber Ä‘á»ƒ xá»­ lÃ½ layout nhiá»u cá»™t\n    \"\"\"\n    full_text = []\n    \n    with pdfplumber.open(pdf_path) as pdf:\n        for page_num, page in enumerate(pdf.pages, 1):\n            # Láº¥y kÃ­ch thÆ°á»›c page\n            width = page.width\n            height = page.height\n            \n            # Chia page thÃ nh cÃ¡c cá»™t\n            if column_count == 2:\n                # Cá»™t trÃ¡i\n                left_bbox = (0, 0, width / 2, height)\n                left_text = page.within_bbox(left_bbox).extract_text()\n                \n                # Cá»™t pháº£i\n                right_bbox = (width / 2, 0, width, height)\n                right_text = page.within_bbox(right_bbox).extract_text()\n                \n                result = []\n                if left_text:\n                    result.append(\"=== Cá»˜T TRÃI ===\")\n                    result.append(left_text)\n                if right_text:\n                    result.append(\"\\n=== Cá»˜T PHáº¢I ===\")\n                    result.append(right_text)\n                \n                full_text.append(f\"--- Trang {page_num} ---\\n\" + \"\\n\".join(result))\n            \n            elif column_count == 3:\n                third = width / 3\n                \n                left_text = page.within_bbox((0, 0, third, height)).extract_text()\n                middle_text = page.within_bbox((third, 0, 2*third, height)).extract_text()\n                right_text = page.within_bbox((2*third, 0, width, height)).extract_text()\n                \n                result = []\n                for name, text in [(\"TRÃI\", left_text), (\"GIá»®A\", middle_text), (\"PHáº¢I\", right_text)]:\n                    if text:\n                        result.append(f\"\\n=== Cá»˜T {name} ===\")\n                        result.append(text)\n                \n                full_text.append(f\"--- Trang {page_num} ---\\n\" + \"\\n\".join(result))\n    \n    return \"\\n\\n\".join(full_text)\n\n\n# ============================================================================\n# GIáº¢I PHÃP Tá»I Æ¯U: Káº¾T Há»¢P VÃ€ LÃ€M Sáº CH\n# ============================================================================\n\ndef parse_cv_with_column_detection(pdf_path, method='smart', keep_original=True):\n    \"\"\"\n    Parse CV vá»›i phÃ¡t hiá»‡n cá»™t tá»± Ä‘á»™ng vÃ  lÃ m sáº¡ch\n    \n    Args:\n        pdf_path: ÄÆ°á»ng dáº«n file PDF\n        method: 'auto', 'smart', 'pdfplumber'\n        keep_original: Náº¿u True, giá»¯ nguyÃªn text khi khÃ´ng phÃ¡t hiá»‡n Ä‘Æ°á»£c sections\n    \"\"\"\n    # 1. TrÃ­ch xuáº¥t vá»›i phÃ¡t hiá»‡n cá»™t\n    if method == 'smart':\n        raw_text = extract_pdf_smart_layout(pdf_path)\n    elif method == 'pdfplumber':\n        raw_text = extract_pdf_pdfplumber_columns(pdf_path, column_count=2)\n    else:  # auto\n        raw_text = extract_pdf_with_column_detection(pdf_path, column_count='auto')\n    \n    # 2. LÃ m sáº¡ch text\n    cleaned_text = clean_extracted_text(raw_text)\n    \n    # 3. Sáº¯p xáº¿p láº¡i thÃ nh single column logic\n    structured_text = restructure_to_single_column(cleaned_text, keep_original_if_no_sections=keep_original)\n    \n    return {\n        'raw': raw_text,\n        'cleaned': cleaned_text,\n        'structured': structured_text\n    }\n\n\ndef clean_extracted_text(text):\n    \"\"\"LÃ m sáº¡ch text Ä‘Ã£ trÃ­ch xuáº¥t\"\"\"\n    # XÃ³a dáº¥u phÃ¢n cÃ¡ch cá»™t\n    text = re.sub(r'=== Cá»˜T (TRÃI|PHáº¢I|GIá»®A) ===', '', text)\n    text = re.sub(r'=== (HEADER|Ná»˜I DUNG CHÃNH|THÃ”NG TIN BÃŠN) ===', '', text)\n    \n    # XÃ³a dáº¥u phÃ¢n trang\n    text = re.sub(r'--- Trang \\d+ ---', '', text)\n    \n    # XÃ³a khoáº£ng tráº¯ng thá»«a\n    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n    text = re.sub(r' +', ' ', text)\n    \n    return text.strip()\n\n\ndef restructure_to_single_column(text, keep_original_if_no_sections=True):\n    \"\"\"\n    Sáº¯p xáº¿p láº¡i text thÃ nh single column theo logic CV\n    Thá»© tá»±: TÃªn â†’ Contact â†’ Má»¥c tiÃªu â†’ Há»c váº¥n â†’ Kinh nghiá»‡m â†’ Ká»¹ nÄƒng â†’ Dá»± Ã¡n\n    \n    Args:\n        text: Text Ä‘Ã£ trÃ­ch xuáº¥t\n        keep_original_if_no_sections: Náº¿u True, giá»¯ nguyÃªn text gá»‘c khi khÃ´ng tÃ¬m tháº¥y sections\n    \"\"\"\n    sections = {\n        'header': [],\n        'contact': [],\n        'objective': [],\n        'education': [],\n        'experience': [],\n        'skills': [],\n        'projects': [],\n        'certifications': [],\n        'languages': [],\n        'awards': [],\n        'other': []\n    }\n    \n    # Keywords má»Ÿ rá»™ng hÆ¡n - há»— trá»£ nhiá»u format CV\n    section_keywords = {\n        'objective': [\n            'má»¥c tiÃªu', 'objective', 'career objective', 'mong muá»‘n',\n            'career goal', 'professional summary', 'tÃ³m táº¯t'\n        ],\n        'education': [\n            'há»c váº¥n', 'education', 'Ä‘áº¡i há»c', 'university', 'college',\n            'trÆ°á»ng', 'training', 'academic', 'qualification'\n        ],\n        'experience': [\n            'kinh nghiá»‡m', 'experience', 'work experience', 'employment',\n            'lÃ m viá»‡c', 'work history', 'professional experience',\n            'cÃ´ng ty', 'company', 'thá»±c táº­p', 'internship'\n        ],\n        'skills': [\n            'ká»¹ nÄƒng', 'skills', 'technical skills', 'competencies',\n            'expertise', 'abilities', 'chuyÃªn mÃ´n'\n        ],\n        'projects': [\n            'dá»± Ã¡n', 'projects', 'portfolio', 'work samples'\n        ],\n        'certifications': [\n            'chá»©ng chá»‰', 'certifications', 'certificates', 'licenses',\n            'báº±ng cáº¥p', 'credentials'\n        ],\n        'languages': [\n            'ngoáº¡i ngá»¯', 'languages', 'tiáº¿ng anh', 'foreign language',\n            'language skills'\n        ],\n        'awards': [\n            'giáº£i thÆ°á»Ÿng', 'awards', 'honors', 'achievements',\n            'recognition', 'thÃ nh tÃ­ch'\n        ]\n    }\n    \n    lines = text.split('\\n')\n    current_section = 'other'\n    section_found_count = 0\n    \n    # Track xem cÃ³ tÃ¬m tháº¥y Ã­t nháº¥t 1 keyword section khÃ´ng\n    for line in lines:\n        line_stripped = line.strip()\n        if not line_stripped:\n            continue\n        \n        line_lower = line_stripped.lower()\n        \n        # Kiá»ƒm tra xem dÃ²ng nÃ y cÃ³ pháº£i lÃ  section header khÃ´ng\n        is_section_header = False\n        \n        # Æ¯u tiÃªn: dÃ²ng ngáº¯n, in hoa, hoáº·c cÃ³ format Ä‘áº·c biá»‡t\n        is_potential_header = (\n            len(line_stripped) < 60 and \n            (line_stripped.isupper() or \n             line_stripped.count(' ') < 5 or\n             line_stripped.startswith('#'))\n        )\n        \n        # PhÃ¡t hiá»‡n section dá»±a trÃªn keywords\n        for section_name, keywords in section_keywords.items():\n            if any(kw in line_lower for kw in keywords):\n                # Náº¿u lÃ  dÃ²ng ngáº¯n/header format â†’ Ä‘Ã¢y lÃ  section header\n                if is_potential_header:\n                    current_section = section_name\n                    section_found_count += 1\n                    is_section_header = True\n                    break\n                # Náº¿u khÃ´ng pháº£i header nhÆ°ng cÃ³ keyword â†’ váº«n chuyá»ƒn section\n                elif len([kw for kw in keywords if kw in line_lower]) >= 1:\n                    current_section = section_name\n                    section_found_count += 1\n        \n        # KhÃ´ng thÃªm dÃ²ng section header vÃ o káº¿t quáº£\n        if is_section_header:\n            continue\n        \n        # PhÃ¡t hiá»‡n contact info (email, phone, address)\n        has_phone = bool(re.search(r'0\\d{9,10}', line_stripped))\n        has_email = '@' in line_stripped and '.' in line_stripped\n        has_address_keywords = any(kw in line_lower for kw in [\n            'hÃ  ná»™i', 'há»“ chÃ­ minh', 'Ä‘Ã  náºµng', 'quáº­n', 'phÆ°á»ng', \n            'district', 'ward', 'street', 'address'\n        ])\n        \n        if has_phone or has_email or has_address_keywords:\n            # Náº¿u chÆ°a cÃ³ section nÃ o, Ä‘Æ°a vÃ o contact\n            if current_section == 'other' or current_section == 'header':\n                current_section = 'contact'\n        \n        # ThÃªm vÃ o section tÆ°Æ¡ng á»©ng\n        sections[current_section].append(line_stripped)\n    \n    # Náº¿u khÃ´ng tÃ¬m tháº¥y sections vÃ  flag = True â†’ giá»¯ nguyÃªn text gá»‘c\n    if section_found_count < 2 and keep_original_if_no_sections:\n        return text\n    \n    # GhÃ©p láº¡i theo thá»© tá»± chuáº©n\n    result = []\n    \n    # Header (thÆ°á»ng lÃ  tÃªn, title)\n    if sections['header']:\n        result.extend(sections['header'])\n        result.append('')\n    \n    # Contact info\n    if sections['contact']:\n        result.append('## THÃ”NG TIN LIÃŠN Há»†')\n        result.extend(sections['contact'])\n        result.append('')\n    \n    # Main sections theo thá»© tá»± logic\n    section_order = [\n        ('objective', '## Má»¤C TIÃŠU NGHá»€ NGHIá»†P'),\n        ('education', '## Há»ŒC Váº¤N'),\n        ('experience', '## KINH NGHIá»†M LÃ€M VIá»†C'),\n        ('skills', '## Ká»¸ NÄ‚NG'),\n        ('projects', '## Dá»° ÃN'),\n        ('certifications', '## CHá»¨NG CHá»ˆ'),\n        ('languages', '## NGOáº I NGá»®'),\n        ('awards', '## GIáº¢I THÆ¯á»NG')\n    ]\n    \n    for section_name, section_title in section_order:\n        if sections[section_name]:\n            result.append(section_title)\n            result.extend(sections[section_name])\n            result.append('')\n    \n    # Other - nhá»¯ng pháº§n khÃ´ng phÃ¢n loáº¡i Ä‘Æ°á»£c\n    if sections['other']:\n        result.append('## THÃ”NG TIN KHÃC')\n        result.extend(sections['other'])\n    \n    return '\\n'.join(result)\n\n\n# ============================================================================\n# Sá»¬ Dá»¤NG - VÃ Dá»¤ CHO FILE CV Cá»¦A Báº N\n# ============================================================================\n\npdf_file = \"/kaggle/input/cv-huy/nguyen-duc-huy_1758517966_Joboko_c3e1a50bcfd6fb7f_3487225.pdf\"\n\nprint(\"=== PHÆ¯Æ NG PHÃP 1: AUTO DETECT ===\")\nresult1 = parse_cv_with_column_detection(pdf_file, method='auto')\nprint(result1['structured'])\n\nprint(\"\\n\\n=== PHÆ¯Æ NG PHÃP 2: SMART LAYOUT ===\")\nresult2 = parse_cv_with_column_detection(pdf_file, method='smart')\nprint(result2['structured'])\n\n\n# LÆ°u káº¿t quáº£ tá»‘t nháº¥t\nbest_result = result2  # ThÆ°á»ng smart layout cho káº¿t quáº£ tá»‘t nháº¥t\n\n\n# So sÃ¡nh káº¿t quáº£\nprint(\"\\n=== SO SÃNH PHÆ¯Æ NG PHÃP ===\")\nprint(f\"Auto detect: {len(result1['structured'])} kÃ½ tá»±\")\nprint(f\"Smart layout: {len(result2['structured'])} kÃ½ tá»±\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:58:05.841856Z","iopub.execute_input":"2025-10-08T05:58:05.842475Z","iopub.status.idle":"2025-10-08T05:58:06.396412Z","shell.execute_reply.started":"2025-10-08T05:58:05.842453Z","shell.execute_reply":"2025-10-08T05:58:06.395681Z"}},"outputs":[{"name":"stdout","text":"=== PHÆ¯Æ NG PHÃP 1: AUTO DETECT ===\n## THÃ”NG TIN LIÃŠN Há»†\n0962865038\n25/01/2003\nNam\nhuydcb3@gmail.com\n25/01/2003\nMai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i\nD á»° Ã N\n- XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2.\n- XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean.\n- PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\" qua áº£nh chá»¥p.\nC H á»¨N G C H á»ˆ\nTOIEC (735/990) 2024\nAI ENGINEER NGUYá»„N Äá»¨C HUY\n\n## Há»ŒC Váº¤N\nKhoa há»c mÃ¡y tÃ­nh\n08/2021-06/2025\nGPA 3.2\n\n## KINH NGHIá»†M LÃ€M VIá»†C\nT I áº¾N G A N H\nNghe Äá»c\nÆ¯ U Ä I á»‚M\nTÃ­nh ká»· luáº­t cao: tuÃ¢n thá»§ quy Ä‘á»‹nh khi lÃ m viá»‡c. TÃ¬m kiáº¿m thÃ´ng tin vÃ  giáº£i quyáº¿t váº¥n Ä‘á». ThÃ­ch há»c há»i ThÃ nh tháº¡o sá»­ dá»¥ng cÃ´ng cá»¥ AI: Chatgpt, Gemini, Cursor,...\nL I ÃŠ N K áº¾T\nhttps://github.com/Huypluie/Huy/tree /develop\nDev tool Python\n01/2022-03/2025\n- Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng . Nhá»¯ng Ä‘iá»u Ä‘áº¡t Ä‘Æ°á»£c: - Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á» - ÄÆ°á»£c tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p.\nÂ© Joboko.com\n\n## Ká»¸ NÄ‚NG\nHá»‡ Ä‘iá»u hÃ nh Window, Linux\nNgÃ´n ngá»¯ láº­p trÃ¬nh Python, C++\nService Pytorch, Tensorflow\nCÃ´ng cá»¥ DevOps Git\nCloud Google Colab, Kaggle Notebooks\n\n## Dá»° ÃN\nTÃ´i mong muá»‘n á»©ng tuyá»ƒn vÃ o vá»‹ trÃ­ Intern AI Engineer táº¡i cÃ´ng ty. LÃ  má»™t ngÆ°á»i cÃ³ tÆ° duy ká»¹ thuáº­t, thuáº­t toÃ¡n tá»‘t vÃ  há»c há»i cÃ´ng nghá»‡ má»›i nhanh, tÃ´i cÃ³ niá»m Ä‘am mÃª sÃ¢u sáº¯c vá»›i lÄ©nh vá»±c cÃ´ng nghá»‡ Ä‘áº·c biá»‡t lÃ  AI. TÃ´i ráº¥t hy vá»ng tÃ´i cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c lÃ m viá»‡c vÃ  cá»‘ng hiáº¿n táº¡i cÃ´ng ty. TÃ´i sáº½ cá»‘ gáº¯ng ná»— lá»±c há»c há»i tá»« mÃ´i trÆ°á»ng chuyÃªn nghiá»‡p, tÃ­ch lÅ©y kinh nghiá»‡m thá»±c táº¿ vÃ  tá»«ng bÆ°á»›c phÃ¡t triá»ƒn nÄƒng lá»±c chuyÃªn mÃ´n. Má»¥c tiÃªu trong 3-4 nÄƒm tá»›i cá»§a tÃ´i lÃ  trá»Ÿ thÃ nh má»™t AI Engineer cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» láº­p trÃ¬nh, xá»­ lÃ½ dá»¯ liá»‡u vÃ  triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y.TÃ´i mong muá»‘n Ä‘Æ°á»£c tham gia vÃ o cÃ¡c dá»± Ã¡n AI thá»±c táº¿ Ä‘á»ƒ tÃ­ch lÅ©y kinh nghiá»‡m, tá»«ng bÆ°á»›c hoÃ n thiá»‡n ká»¹ nÄƒng xÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh AI, hÆ°á»›ng tá»›i vai trÃ² chuyÃªn sÃ¢u trong lÄ©nh vá»±c TrÃ­ tuá»‡ nhÃ¢n táº¡o.\n\n\n\n=== PHÆ¯Æ NG PHÃP 2: SMART LAYOUT ===\n## Há»ŒC Váº¤N\nKhoa há»c mÃ¡y tÃ­nh\n08/2021-06/2025\nGPA 3.2\n\n## KINH NGHIá»†M LÃ€M VIá»†C\n0962865038\n25/01/2003\nNam\nhuydcb3@gmail.com\n25/01/2003\nMai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i\nD á»° Ã N\n- XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2.\n- XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean.\n- PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\" qua áº£nh chá»¥p.\nC H á»¨N G C H á»ˆ\nTOIEC (735/990) 2024\nT I áº¾N G A N H\nDev tool Python\n01/2022-03/2025\nNghe Äá»c\n- Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng . Nhá»¯ng Ä‘iá»u Ä‘áº¡t Ä‘Æ°á»£c: - Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á» - ÄÆ°á»£c tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p.\nÆ¯ U Ä I á»‚M\nÂ© Joboko.com\nTÃ­nh ká»· luáº­t cao: tuÃ¢n thá»§ quy Ä‘á»‹nh khi lÃ m viá»‡c. TÃ¬m kiáº¿m thÃ´ng tin vÃ  giáº£i quyáº¿t váº¥n Ä‘á». ThÃ­ch há»c há»i ThÃ nh tháº¡o sá»­ dá»¥ng cÃ´ng cá»¥ AI: Chatgpt, Gemini, Cursor,...\nL I ÃŠ N K áº¾T\nhttps://github.com/Huypluie/Huy/tree /develop\n\n## Ká»¸ NÄ‚NG\nHá»‡ Ä‘iá»u hÃ nh Window, Linux\nNgÃ´n ngá»¯ láº­p trÃ¬nh Python, C++\nService Pytorch, Tensorflow\nCÃ´ng cá»¥ DevOps Git\nCloud Google Colab, Kaggle Notebooks\n\n## Dá»° ÃN\nTÃ´i mong muá»‘n á»©ng tuyá»ƒn vÃ o vá»‹ trÃ­ Intern AI Engineer táº¡i cÃ´ng ty. LÃ  má»™t ngÆ°á»i cÃ³ tÆ° duy ká»¹ thuáº­t, thuáº­t toÃ¡n tá»‘t vÃ  há»c há»i cÃ´ng nghá»‡ má»›i nhanh, tÃ´i cÃ³ niá»m Ä‘am mÃª sÃ¢u sáº¯c vá»›i lÄ©nh vá»±c cÃ´ng nghá»‡ Ä‘áº·c biá»‡t lÃ  AI. TÃ´i ráº¥t hy vá»ng tÃ´i cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c lÃ m viá»‡c vÃ  cá»‘ng hiáº¿n táº¡i cÃ´ng ty. TÃ´i sáº½ cá»‘ gáº¯ng ná»— lá»±c há»c há»i tá»« mÃ´i trÆ°á»ng chuyÃªn nghiá»‡p, tÃ­ch lÅ©y kinh nghiá»‡m thá»±c táº¿ vÃ  tá»«ng bÆ°á»›c phÃ¡t triá»ƒn nÄƒng lá»±c chuyÃªn mÃ´n. Má»¥c tiÃªu trong 3-4 nÄƒm tá»›i cá»§a tÃ´i lÃ  trá»Ÿ thÃ nh má»™t AI Engineer cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» láº­p trÃ¬nh, xá»­ lÃ½ dá»¯ liá»‡u vÃ  triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y.TÃ´i mong muá»‘n Ä‘Æ°á»£c tham gia vÃ o cÃ¡c dá»± Ã¡n AI thá»±c táº¿ Ä‘á»ƒ tÃ­ch lÅ©y kinh nghiá»‡m, tá»«ng bÆ°á»›c hoÃ n thiá»‡n ká»¹ nÄƒng xÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh AI, hÆ°á»›ng tá»›i vai trÃ² chuyÃªn sÃ¢u trong lÄ©nh vá»±c TrÃ­ tuá»‡ nhÃ¢n táº¡o.\n\n## THÃ”NG TIN KHÃC\nAI ENGINEER NGUYá»„N Äá»¨C HUY\n\n=== SO SÃNH PHÆ¯Æ NG PHÃP ===\nAuto detect: 1898 kÃ½ tá»±\nSmart layout: 1894 kÃ½ tá»±\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!apt-get update -qq\n!apt-get install -y poppler-utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:53:35.125297Z","iopub.execute_input":"2025-10-21T12:53:35.125873Z","iopub.status.idle":"2025-10-21T12:53:53.224549Z","shell.execute_reply.started":"2025-10-21T12:53:35.125845Z","shell.execute_reply":"2025-10-21T12:53:53.223768Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 158 not upgraded.\nNeed to get 1,470 kB of archives.\nAfter this operation, 697 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.11 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.11 [5,184 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.11 [1,080 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.11 [186 kB]\nFetched 1,470 kB in 0s (4,372 kB/s)    \n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.11_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.11) over (22.02.0-2ubuntu0.8) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.11_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.11) over (22.02.0-2ubuntu0.8) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.11_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.11) over (22.02.0-2ubuntu0.8) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.11_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.11) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.11) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.11) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.11) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.11) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -y transformers\n!pip install transformers==4.42.3\n!pip install pdfminer.six pdf2image --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:50:23.848070Z","iopub.execute_input":"2025-10-21T12:50:23.848301Z","iopub.status.idle":"2025-10-21T12:50:45.700279Z","shell.execute_reply.started":"2025-10-21T12:50:23.848284Z","shell.execute_reply":"2025-10-21T12:50:45.699344Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nCollecting transformers==4.42.3\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.42.3)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (0.5.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.42.3)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.3) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.3) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.3) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.3) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17->transformers==4.42.3) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.3) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.3) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.3) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17->transformers==4.42.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17->transformers==4.42.3) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.17->transformers==4.42.3) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.17->transformers==4.42.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.17->transformers==4.42.3) (2024.2.0)\nDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 tokenizers-0.19.1 transformers-4.42.3\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n\nimport transformers, torch, huggingface_hub\nprint(\"Transformers:\", transformers.__version__)\nprint(\"Torch:\", torch.__version__)\nprint(\"Huggingface Hub:\", huggingface_hub.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:51:09.159553Z","iopub.execute_input":"2025-10-21T12:51:09.160258Z","iopub.status.idle":"2025-10-21T12:51:15.694458Z","shell.execute_reply.started":"2025-10-21T12:51:09.160230Z","shell.execute_reply":"2025-10-21T12:51:15.693691Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.42.3\nTorch: 2.6.0+cu124\nHuggingface Hub: 0.35.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import LayoutLMv3Processor, LayoutLMv3Model\nfrom pdfminer.high_level import extract_pages\nfrom pdfminer.layout import LTTextContainer, LTChar, LTAnno\nfrom pdf2image import convert_from_path\nfrom PIL import Image\n\n# ==== 1ï¸âƒ£ Load LayoutLMv3 model (chá»‰ Ä‘á»c layout, khÃ´ng cáº§n NER layer) ====\nmodel_name = \"microsoft/layoutlmv3-base\"\nprocessor = LayoutLMv3Processor.from_pretrained(model_name, apply_ocr=False)\nmodel = LayoutLMv3Model.from_pretrained(model_name)\nmodel.eval()\n\n# ==== 2ï¸âƒ£ TrÃ­ch xuáº¥t text + bbox tá»« PDF ====\ndef extract_text_and_bboxes(pdf_path):\n    words, boxes = [], []\n    for page_layout in extract_pages(pdf_path):\n        (page_x0, page_y0, page_x1, page_y1) = page_layout.bbox\n        page_height = page_y1 - page_y0\n\n        for element in page_layout:\n            if isinstance(element, LTTextContainer):\n                for text_line in element:\n                    line_text = \"\"\n                    line_x0, line_y0, line_x1, line_y1 = text_line.bbox\n                    for character in text_line:\n                        if isinstance(character, LTChar):\n                            line_text += character.get_text()\n                    words_in_line = line_text.strip().split()\n                    if not words_in_line:\n                        continue\n\n                    # bounding box normalized 0â€“1000\n                    x0 = int(line_x0 / page_x1 * 1000)\n                    y0 = int(line_y0 / page_y1 * 1000)\n                    x1 = int(line_x1 / page_x1 * 1000)\n                    y1 = int(line_y1 / page_y1 * 1000)\n\n                    for word in words_in_line:\n                        words.append(word)\n                        boxes.append([x0, y0, x1, y1])\n    return words, boxes\n\n# ==== 3ï¸âƒ£ Chuáº©n bá»‹ input cho LayoutLMv3 ====\ndef prepare_inputs_from_pdf(pdf_path):\n    words, boxes = extract_text_and_bboxes(pdf_path)\n\n    # Láº¥y trang Ä‘áº§u Ä‘á»ƒ cÃ³ áº£nh ná»n\n    image = convert_from_path(pdf_path)[0].convert(\"RGB\")\n\n    encoding = processor(\n        images=image,\n        text=words,\n        boxes=boxes,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n    )\n    return encoding, words\n\n# ==== 4ï¸âƒ£ Cháº¡y mÃ´ hÃ¬nh ====\ndef read_pdf_with_layout(pdf_path):\n    encoding, words = prepare_inputs_from_pdf(pdf_path)\n    with torch.no_grad():\n        outputs = model(**encoding)\n    return words, outputs.last_hidden_state  # má»—i token 768-dim vector biá»ƒu diá»…n ná»™i dung + bá»‘ cá»¥c\n\n# ==== 5ï¸âƒ£ Test ====\npdf_path = \"/kaggle/input/cv-huy/nguyen-duc-huy_1758517966_Joboko_c3e1a50bcfd6fb7f_3487225.pdf\"\nwords, embeddings = read_pdf_with_layout(pdf_path)\n\nprint(\"ğŸ“„ Tá»•ng sá»‘ tá»«:\", len(words))\nprint(\"ğŸ”¹ text:\")\nprint(words)\nprint(\"\\nğŸ”¹ Vector embedding shape:\", embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:54:06.912149Z","iopub.execute_input":"2025-10-21T12:54:06.912860Z","iopub.status.idle":"2025-10-21T12:54:11.607207Z","shell.execute_reply.started":"2025-10-21T12:54:06.912831Z","shell.execute_reply":"2025-10-21T12:54:11.606402Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:1060: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“„ Tá»•ng sá»‘ tá»«: 381\nğŸ”¹ text:\n['AI', 'ENGINEER', 'NGUYá»„N', 'Äá»¨C', 'HUY', 'TÃ´i', 'mong', 'muá»‘n', 'á»©ng', 'tuyá»ƒn', 'vÃ o', 'vá»‹', 'trÃ­', 'Intern', 'AI', 'Engineer', 'táº¡i', 'cÃ´ng', 'ty.', 'LÃ ', 'má»™t', 'ngÆ°á»i', 'cÃ³', 'tÆ°', 'duy', 'ká»¹', 'thuáº­t,', 'thuáº­t', 'toÃ¡n', 'tá»‘t', 'vÃ ', 'há»c', 'há»i', 'cÃ´ng', 'nghá»‡', 'má»›i', 'nhanh,', 'tÃ´i', 'cÃ³', 'niá»m', 'Ä‘am', 'mÃª', 'sÃ¢u', 'sáº¯c', 'vá»›i', 'lÄ©nh', 'vá»±c', 'cÃ´ng', 'nghá»‡', 'Ä‘áº·c', 'biá»‡t', 'lÃ ', 'AI.', 'TÃ´i', 'ráº¥t', 'hy', 'vá»ng', 'tÃ´i', 'cÃ³', 'cÆ¡', 'há»™i', 'Ä‘Æ°á»£c', 'lÃ m', 'viá»‡c', 'vÃ ', 'cá»‘ng', 'hiáº¿n', 'táº¡i', 'cÃ´ng', 'ty.', 'TÃ´i', 'sáº½', 'cá»‘', 'gáº¯ng', 'ná»—', 'lá»±c', 'há»c', 'há»i', 'tá»«', 'mÃ´i', 'trÆ°á»ng', 'chuyÃªn', 'nghiá»‡p,', 'tÃ­ch', 'lÅ©y', 'kinh', 'nghiá»‡m', 'thá»±c', 'táº¿', 'vÃ ', 'tá»«ng', 'bÆ°á»›c', 'phÃ¡t', 'triá»ƒn', 'nÄƒng', 'lá»±c', 'chuyÃªn', 'mÃ´n.', 'Má»¥c', 'tiÃªu', 'trong', '3-4', 'nÄƒm', 'tá»›i', 'cá»§a', 'tÃ´i', 'lÃ ', 'trá»Ÿ', 'thÃ nh', 'má»™t', 'AI', 'Engineer', 'cÃ³', 'ná»n', 'táº£ng', 'vá»¯ng', 'cháº¯c', 'vá»', 'láº­p', 'trÃ¬nh,', 'xá»­', 'lÃ½', 'dá»¯', 'liá»‡u', 'vÃ ', 'triá»ƒn', 'khai', 'mÃ´', 'hÃ¬nh', 'há»c', 'mÃ¡y.TÃ´i', 'mong', 'muá»‘n', 'Ä‘Æ°á»£c', 'tham', 'gia', 'vÃ o', 'cÃ¡c', 'dá»±', 'Ã¡n', 'AI', 'thá»±c', 'táº¿', 'Ä‘á»ƒ', 'tÃ­ch', 'lÅ©y', 'kinh', 'nghiá»‡m,', 'tá»«ng', 'bÆ°á»›c', 'hoÃ n', 'thiá»‡n', 'ká»¹', 'nÄƒng', 'xÃ¢y', 'dá»±ng,', 'huáº¥n', 'luyá»‡n', 'vÃ ', 'triá»ƒn', 'khai', 'mÃ´', 'hÃ¬nh', 'AI,', 'hÆ°á»›ng', 'tá»›i', 'vai', 'trÃ²', 'chuyÃªn', 'sÃ¢u', 'trong', 'lÄ©nh', 'vá»±c', 'TrÃ­', 'tuá»‡', 'nhÃ¢n', 'táº¡o.', 'Há»ŒC', 'Váº¤N', 'TrÆ°á»ng', 'Äáº¡i', 'há»c', 'CÃ´ng', 'Nghiá»‡p', 'Khoa', 'há»c', 'mÃ¡y', 'tÃ­nh', '08/2021-06/2025', 'GPA', '3.2', 'Ká»¸', 'NÄ‚NG', 'Há»‡', 'Ä‘iá»u', 'hÃ nh', 'Window,', 'Linux', 'NgÃ´n', 'ngá»¯', 'láº­p', 'trÃ¬nh', 'Python,', 'C++', 'Service', 'Pytorch,', 'Tensor\\x00ow', 'CÃ´ng', 'cá»¥', 'DevOps', 'Git', 'Cloud', 'Google', 'Colab,', 'Kaggle', 'Notebooks', 'KINH', 'NGHIá»†M', 'LÃ€M', 'VIá»†C', 'CÃ”NG', 'TY', 'TNHH', 'HEXAGON', '0962865038', '25/01/2003', 'Nam', 'huydcb3@gmail.com', '25/01/2003', 'Mai', 'Dá»‹ch,', 'Cáº§u', 'Giáº¥y,', 'HÃ ', 'Ná»™i', 'Dá»°', 'ÃN', '-', 'XÃ¢y', 'dá»±ng', 'mÃ´', 'hÃ¬nh', 'há»—', 'trá»£', 'hoÃ n', 'thiá»‡n', 'cÃ¢u', 'vá»›i', 'GPT-2.', '-', 'XÃ¢y', 'dá»±ng', 'há»‡', 'thá»‘ng', 'cáº£nh', 'bÃ¡o', 'ngá»§', 'gáº­t', 'trÃªn', 'xe', 'vá»›i', 'Kmean.', '-', 'PhÃ¡t', 'triá»ƒn', 'há»‡', 'thá»‘ng', 'há»—', 'trá»£', 'dá»±', 'Ä‘oÃ¡n', 'bá»‡nh', '\"Tiá»ƒu', 'Ä‘Æ°á»ng', 'giÃ¡c', 'máº¡c\"', 'qua', 'áº£nh', 'chá»¥p.', 'CHá»¨NG', 'CHá»ˆ', 'TOIEC', '(735/990)', '2024', 'Dev', 'tool', 'Python', '01/2022-03/2025', '-Viáº¿t', 'tool', 'há»—', 'trá»£', 'tá»±', 'Ä‘á»™ng', 'hÃ³a', 'cÃ¡c', 'chá»©c', 'nÄƒng', '.', 'Nhá»¯ng', 'Ä‘iá»u', 'Ä‘áº¡t', 'Ä‘Æ°á»£c:', '-', 'Há»c', 'há»i', 'Ä‘Æ°á»£c', 'kÄ©', 'nÄƒng', 'láº­p', 'trÃ¬nh,', 'lÃ m', 'viá»‡c', 'nhÃ³m', 'vÃ ', 'tá»±', 'giáº£i', 'quyáº¿t', 'váº¥n', 'Ä‘á»', '-ÄÆ°á»£c', 'tiáº¿p', 'cáº­n', 'há»c', 'há»i', 'cÃ¡c', 'cÃ´ng', 'nghá»‡', 'má»›i,', 'cÃ¡c', 'bÃ i', 'toÃ¡n', 'nghiá»‡p', 'vá»¥', 'phá»©c', 'táº¡p.', 'TIáº¾NG', 'ANH', 'Nghe', 'Äá»c', 'Æ¯U', 'ÄIá»‚M', 'TÃ­nh', 'ká»·', 'luáº­t', 'cao:', 'tuÃ¢n', 'thá»§', 'quy', 'Ä‘á»‹nh', 'khi', 'lÃ m', 'viá»‡c.', 'TÃ¬m', 'kiáº¿m', 'thÃ´ng', 'tin', 'vÃ ', 'giáº£i', 'quyáº¿t', 'váº¥n', 'Ä‘á».', 'ThÃ­ch', 'há»c', 'há»i', 'ThÃ nh', 'tháº¡o', 'sá»­', 'dá»¥ng', 'cÃ´ng', 'cá»¥', 'AI:', 'Chatgpt,', 'Gemini,', 'Cursor,...', 'LIÃŠN', 'Káº¾T', 'https://github.com/Huypluie/Huy/tree', '/develop', 'Â©', 'Joboko.com']\n\nğŸ”¹ Vector embedding shape: torch.Size([1, 709, 768])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import re\ntext = \" \".join(words)\n\n# 2ï¸âƒ£ Loáº¡i cÃ¡c kÃ½ tá»± null hoáº·c control lá»—i\ntext = text.replace(\"\\x00\", \"\")\n\n# 3ï¸âƒ£ Chuáº©n hoÃ¡ khoáº£ng tráº¯ng\ntext = re.sub(r\"\\s+\", \" \", text).strip()\n\nprint(text)  # in thá»­ 1000 kÃ½ tá»± Ä‘áº§u","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T12:55:05.872583Z","iopub.execute_input":"2025-10-21T12:55:05.873347Z","iopub.status.idle":"2025-10-21T12:55:05.878189Z","shell.execute_reply.started":"2025-10-21T12:55:05.873321Z","shell.execute_reply":"2025-10-21T12:55:05.877499Z"}},"outputs":[{"name":"stdout","text":"AI ENGINEER NGUYá»„N Äá»¨C HUY TÃ´i mong muá»‘n á»©ng tuyá»ƒn vÃ o vá»‹ trÃ­ Intern AI Engineer táº¡i cÃ´ng ty. LÃ  má»™t ngÆ°á»i cÃ³ tÆ° duy ká»¹ thuáº­t, thuáº­t toÃ¡n tá»‘t vÃ  há»c há»i cÃ´ng nghá»‡ má»›i nhanh, tÃ´i cÃ³ niá»m Ä‘am mÃª sÃ¢u sáº¯c vá»›i lÄ©nh vá»±c cÃ´ng nghá»‡ Ä‘áº·c biá»‡t lÃ  AI. TÃ´i ráº¥t hy vá»ng tÃ´i cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c lÃ m viá»‡c vÃ  cá»‘ng hiáº¿n táº¡i cÃ´ng ty. TÃ´i sáº½ cá»‘ gáº¯ng ná»— lá»±c há»c há»i tá»« mÃ´i trÆ°á»ng chuyÃªn nghiá»‡p, tÃ­ch lÅ©y kinh nghiá»‡m thá»±c táº¿ vÃ  tá»«ng bÆ°á»›c phÃ¡t triá»ƒn nÄƒng lá»±c chuyÃªn mÃ´n. Má»¥c tiÃªu trong 3-4 nÄƒm tá»›i cá»§a tÃ´i lÃ  trá»Ÿ thÃ nh má»™t AI Engineer cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» láº­p trÃ¬nh, xá»­ lÃ½ dá»¯ liá»‡u vÃ  triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y.TÃ´i mong muá»‘n Ä‘Æ°á»£c tham gia vÃ o cÃ¡c dá»± Ã¡n AI thá»±c táº¿ Ä‘á»ƒ tÃ­ch lÅ©y kinh nghiá»‡m, tá»«ng bÆ°á»›c hoÃ n thiá»‡n ká»¹ nÄƒng xÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh AI, hÆ°á»›ng tá»›i vai trÃ² chuyÃªn sÃ¢u trong lÄ©nh vá»±c TrÃ­ tuá»‡ nhÃ¢n táº¡o. Há»ŒC Váº¤N TrÆ°á»ng Äáº¡i há»c CÃ´ng Nghiá»‡p Khoa há»c mÃ¡y tÃ­nh 08/2021-06/2025 GPA 3.2 Ká»¸ NÄ‚NG Há»‡ Ä‘iá»u hÃ nh Window, Linux NgÃ´n ngá»¯ láº­p trÃ¬nh Python, C++ Service Pytorch, Tensorow CÃ´ng cá»¥ DevOps Git Cloud Google Colab, Kaggle Notebooks KINH NGHIá»†M LÃ€M VIá»†C CÃ”NG TY TNHH HEXAGON 0962865038 25/01/2003 Nam huydcb3@gmail.com 25/01/2003 Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i Dá»° ÃN - XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2. - XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean. - PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\" qua áº£nh chá»¥p. CHá»¨NG CHá»ˆ TOIEC (735/990) 2024 Dev tool Python 01/2022-03/2025 -Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng . Nhá»¯ng Ä‘iá»u Ä‘áº¡t Ä‘Æ°á»£c: - Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á» -ÄÆ°á»£c tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p. TIáº¾NG ANH Nghe Äá»c Æ¯U ÄIá»‚M TÃ­nh ká»· luáº­t cao: tuÃ¢n thá»§ quy Ä‘á»‹nh khi lÃ m viá»‡c. TÃ¬m kiáº¿m thÃ´ng tin vÃ  giáº£i quyáº¿t váº¥n Ä‘á». ThÃ­ch há»c há»i ThÃ nh tháº¡o sá»­ dá»¥ng cÃ´ng cá»¥ AI: Chatgpt, Gemini, Cursor,... LIÃŠN Káº¾T https://github.com/Huypluie/Huy/tree /develop Â© Joboko.com\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"Qwen/Qwen2-7B-Instruct\"  # hoáº·c Qwen3 náº¿u báº¡n dÃ¹ng Qwen3-7B-Instruct\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=\"auto\",       # tá»± chá»n float16 hoáº·c bfloat16\n    device_map=\"auto\",        # tá»± chia Ä‘á»u qua 2 GPU\n    trust_remote_code=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:43:51.760902Z","iopub.execute_input":"2025-10-21T13:43:51.761632Z","iopub.status.idle":"2025-10-21T13:44:26.596788Z","shell.execute_reply.started":"2025-10-21T13:43:51.761604Z","shell.execute_reply":"2025-10-21T13:44:26.596187Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a890a44043474e90b0fb55f31aa57f"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model.device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:45:37.164315Z","iopub.execute_input":"2025-10-21T13:45:37.165074Z","iopub.status.idle":"2025-10-21T13:45:37.169873Z","shell.execute_reply.started":"2025-10-21T13:45:37.165047Z","shell.execute_reply":"2025-10-21T13:45:37.169084Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"Resumetext = \"\"\"\nAI ENGINEER NGUYá»„N Äá»¨C HUY TÃ´i mong muá»‘n á»©ng tuyá»ƒn vÃ o vá»‹ trÃ­ Intern AI Engineer táº¡i cÃ´ng ty. LÃ  má»™t ngÆ°á»i cÃ³ tÆ° duy ká»¹ thuáº­t, thuáº­t toÃ¡n tá»‘t vÃ  há»c há»i cÃ´ng nghá»‡ má»›i nhanh, tÃ´i cÃ³ niá»m Ä‘am mÃª sÃ¢u sáº¯c vá»›i lÄ©nh vá»±c cÃ´ng nghá»‡ Ä‘áº·c biá»‡t lÃ  AI. TÃ´i ráº¥t hy vá»ng tÃ´i cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c lÃ m viá»‡c vÃ  cá»‘ng hiáº¿n táº¡i cÃ´ng ty. TÃ´i sáº½ cá»‘ gáº¯ng ná»— lá»±c há»c há»i tá»« mÃ´i trÆ°á»ng chuyÃªn nghiá»‡p, tÃ­ch lÅ©y kinh nghiá»‡m thá»±c táº¿ vÃ  tá»«ng bÆ°á»›c phÃ¡t triá»ƒn nÄƒng lá»±c chuyÃªn mÃ´n. Má»¥c tiÃªu trong 3-4 nÄƒm tá»›i cá»§a tÃ´i lÃ  trá»Ÿ thÃ nh má»™t AI Engineer cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» láº­p trÃ¬nh, xá»­ lÃ½ dá»¯ liá»‡u vÃ  triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y.TÃ´i mong muá»‘n Ä‘Æ°á»£c tham gia vÃ o cÃ¡c dá»± Ã¡n AI thá»±c táº¿ Ä‘á»ƒ tÃ­ch lÅ©y kinh nghiá»‡m, tá»«ng bÆ°á»›c hoÃ n thiá»‡n ká»¹ nÄƒng xÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh AI, hÆ°á»›ng tá»›i vai trÃ² chuyÃªn sÃ¢u trong lÄ©nh vá»±c TrÃ­ tuá»‡ nhÃ¢n táº¡o. Há»ŒC Váº¤N TrÆ°á»ng Äáº¡i há»c CÃ´ng Nghiá»‡p Khoa há»c mÃ¡y tÃ­nh 08/2021-06/2025 GPA 3.2 Ká»¸ NÄ‚NG Há»‡ Ä‘iá»u hÃ nh Window, Linux NgÃ´n ngá»¯ láº­p trÃ¬nh Python, C++ Service Pytorch, Tensorow CÃ´ng cá»¥ DevOps Git Cloud Google Colab, Kaggle Notebooks KINH NGHIá»†M LÃ€M VIá»†C CÃ”NG TY TNHH HEXAGON 0962865038 25/01/2003 Nam huydcb3@gmail.com 25/01/2003 Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i Dá»° ÃN - XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2. - XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean. - PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\" qua áº£nh chá»¥p. CHá»¨NG CHá»ˆ TOIEC (735/990) 2024 Dev tool Python 01/2022-03/2025 -Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng . Nhá»¯ng Ä‘iá»u Ä‘áº¡t Ä‘Æ°á»£c: - Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á» -ÄÆ°á»£c tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p. TIáº¾NG ANH Nghe Äá»c Æ¯U ÄIá»‚M TÃ­nh ká»· luáº­t cao: tuÃ¢n thá»§ quy Ä‘á»‹nh khi lÃ m viá»‡c. TÃ¬m kiáº¿m thÃ´ng tin vÃ  giáº£i quyáº¿t váº¥n Ä‘á». ThÃ­ch há»c há»i ThÃ nh tháº¡o sá»­ dá»¥ng cÃ´ng cá»¥ AI: Chatgpt, Gemini, Cursor,... LIÃŠN Káº¾T https://github.com/Huypluie/Huy/tree /develop Â© Joboko.com\n\"\"\"\nprompt = f\"\"\"\nYou are an information extraction assistant.\n\nExtract ONLY the following personal information from the resume text below.\nReturn only a valid JSON object in this exact format:\n\n{{\n  \"Full_Name\": \"\",\n  \"Date_of_Birth\": \"\",\n  \"Email\": \"\",\n  \"Phone_Number\": \"\",\n  \"Address\": \"\"\n}}\n\nRules:\n1. \"Full_Name\": extract only the person's actual name, excluding any job title (e.g., if the text says \"AI Engineer Nguyen Duc Huy\", return \"Nguyen Duc Huy\").\n2. \"Date_of_Birth\": must be only a valid date in formats like DD/MM/YYYY or DD-MM-YYYY.\n   - If a date appears immediately before an address, take ONLY the date part for \"Date_of_Birth\".\n3. \"Email\": must contain \"@\" and a valid domain.\n4. \"Phone_Number\": must contain at least 9 digits. Do not include spaces or symbols ,giá»¯ nguyÃªn khÃ´ng chuyá»ƒn thÃ nh dáº¡ng miá»n quá»‘c gia nhÆ° +84 .\n5. \"Address\": extract only the location (city, district, street, etc.), without any date or phone number.\n   - For example, if the text says â€œ25/01/2003 Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™iâ€, return:\n       \"Date_of_Birth\": \"25/01/2003\"\n       \"Address\": \"Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i\"\n6. Do not include any explanations or text outside the JSON.\n\nResume text:\"{Resumetext}\"\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1024)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# âœ… XÃ³a pháº§n code fence / markdown\nresponse = re.sub(r\"```(json)?\", \"\", response).strip()\n\n# âœ… TÃ¬m táº¥t cáº£ cÃ¡c Ä‘oáº¡n JSON trong output\njson_matches = re.findall(r\"\\{[\\s\\S]*?\\}\", response)\n\nif json_matches:\n    # âœ… Láº¥y JSON cuá»‘i cÃ¹ng (thÆ°á»ng lÃ  káº¿t quáº£ tháº­t)\n    clean_json = json_matches[-1].strip()\n    try:\n        parsed_json = json.loads(clean_json)\n        print(json.dumps(parsed_json, ensure_ascii=False, indent=4))\n    except json.JSONDecodeError:\n        print(\"âš ï¸ Found JSON-like text but not valid JSON:\")\n        print(clean_json)\nelse:\n    print(\"âŒ No valid JSON found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:18:49.329423Z","iopub.execute_input":"2025-10-21T13:18:49.329738Z","iopub.status.idle":"2025-10-21T13:18:49.336832Z","shell.execute_reply.started":"2025-10-21T13:18:49.329705Z","shell.execute_reply":"2025-10-21T13:18:49.336071Z"}},"outputs":[{"name":"stdout","text":"{\n    \"Full_Name\": \"NGUYá»„N Äá»¨C HUY\",\n    \"Date_of_Birth\": \"25/01/2003\",\n    \"Email\": \"huydcb3@gmail.com\",\n    \"Phone_Number\": \"+84962865038\",\n    \"Address\": \"Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i\"\n}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"info_dict = {'Full_Name': 'NGUYá»„N Äá»¨C HUY',\n 'Date_of_Birth': '25/01/2003',\n 'Email': 'huydcb3@gmail.com',\n 'Phone_Number': '0962865038',\n 'Address': 'Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:50:38.585789Z","iopub.execute_input":"2025-10-21T13:50:38.586389Z","iopub.status.idle":"2025-10-21T13:50:38.589851Z","shell.execute_reply.started":"2025-10-21T13:50:38.586369Z","shell.execute_reply":"2025-10-21T13:50:38.589133Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import re\nimport json\n\ndef normalize_phone(phone):\n    # Bá» kÃ½ tá»± +, khoáº£ng tráº¯ng, dáº¥u cÃ¡ch\n    phone = re.sub(r'\\D', '', phone)\n    # Náº¿u báº¯t Ä‘áº§u báº±ng 84 thÃ¬ Ä‘á»•i sang 0\n    if phone.startswith(\"84\"):\n        phone = \"0\" + phone[2:]\n    return phone\n\ndef remove_personal_info(text, info_dict):\n    text_original = text\n\n    # 1ï¸âƒ£ Xá»­ lÃ½ tÃªn (khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng, khoáº£ng tráº¯ng)\n    if info_dict.get(\"Full_Name\"):\n        name = re.escape(info_dict[\"Full_Name\"].strip())\n        name_parts = [p for p in info_dict[\"Full_Name\"].split() if p]\n        pattern_name = r\"\\b\" + r\"\\s*\".join(name_parts) + r\"\\b\"\n        text = re.sub(pattern_name, \"\", text, flags=re.IGNORECASE)\n\n    # 2ï¸âƒ£ Email\n    if info_dict.get(\"Email\"):\n        email = re.escape(info_dict[\"Email\"].strip())\n        text = re.sub(email, \"\", text, flags=re.IGNORECASE)\n\n    # 3ï¸âƒ£ Sá»‘ Ä‘iá»‡n thoáº¡i\n    if info_dict.get(\"Phone_Number\"):\n        phone_norm = normalize_phone(info_dict[\"Phone_Number\"])\n        # Regex báº¯t cÃ¡c dáº¡ng 0962 865 038 / +84 962865038 / 84-962-865-038 ...\n        pattern_phone = r\"(\\+?84|0)\\s*[-\\.]?\\s*\" + re.escape(phone_norm[-9:])\n        text = re.sub(pattern_phone, \"\", text, flags=re.IGNORECASE)\n\n    # 4ï¸âƒ£ Äá»‹a chá»‰\n    if info_dict.get(\"Address\"):\n        address = re.escape(info_dict[\"Address\"].strip())\n        # XÃ³a tá»«ng cá»¥m con cá»§a Ä‘á»‹a chá»‰ náº¿u cÃ³\n        for part in re.split(r\"[,\\-]\", info_dict[\"Address\"]):\n            part = part.strip()\n            if len(part) > 3:\n                text = re.sub(re.escape(part), \"\", text, flags=re.IGNORECASE)\n\n    # 5ï¸âƒ£ NgÃ y sinh (25/01/2003, 25-1-2003, v.v.)\n    if info_dict.get(\"Date_of_Birth\"):\n        dob = info_dict[\"Date_of_Birth\"].strip()\n        d, m, y = re.split(r\"[\\/\\-]\", dob)\n        pattern_dob = rf\"\\b0?{d}[\\/\\-]0?{m}[\\/\\-]{y}\\b\"\n        text = re.sub(pattern_dob, \"\", text)\n\n    # 6ï¸âƒ£ Dá»n sáº¡ch láº¡i chuá»—i\n    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n\n    # Náº¿u text khÃ´ng thay Ä‘á»•i â†’ thá»­ xoÃ¡ toÃ n bá»™ key-value dáº¡ng \"Full_Name: ...\"\n    if text == text_original:\n        for val in info_dict.values():\n            if isinstance(val, str) and val.strip():\n                text = re.sub(re.escape(val), \"\", text, flags=re.IGNORECASE)\n\n    return text\n\n\ninfo_dict = json.loads(clean_json)\nprint(info_dict)\nclean_text = remove_personal_info(Resumetext, info_dict)\nprint(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:54:04.105257Z","iopub.execute_input":"2025-10-21T13:54:04.105795Z","iopub.status.idle":"2025-10-21T13:54:04.116903Z","shell.execute_reply.started":"2025-10-21T13:54:04.105771Z","shell.execute_reply":"2025-10-21T13:54:04.115968Z"}},"outputs":[{"name":"stdout","text":"{'Full_Name': 'NGUYá»„N Äá»¨C HUY', 'Date_of_Birth': '25/01/2003', 'Email': 'huydcb3@gmail.com', 'Phone_Number': '+84962865038', 'Address': 'Mai Dá»‹ch, Cáº§u Giáº¥y, HÃ  Ná»™i'}\nAI ENGINEER TÃ´i mong muá»‘n á»©ng tuyá»ƒn vÃ o vá»‹ trÃ­ Intern AI Engineer táº¡i cÃ´ng ty. LÃ  má»™t ngÆ°á»i cÃ³ tÆ° duy ká»¹ thuáº­t, thuáº­t toÃ¡n tá»‘t vÃ  há»c há»i cÃ´ng nghá»‡ má»›i nhanh, tÃ´i cÃ³ niá»m Ä‘am mÃª sÃ¢u sáº¯c vá»›i lÄ©nh vá»±c cÃ´ng nghá»‡ Ä‘áº·c biá»‡t lÃ  AI. TÃ´i ráº¥t hy vá»ng tÃ´i cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c lÃ m viá»‡c vÃ  cá»‘ng hiáº¿n táº¡i cÃ´ng ty. TÃ´i sáº½ cá»‘ gáº¯ng ná»— lá»±c há»c há»i tá»« mÃ´i trÆ°á»ng chuyÃªn nghiá»‡p, tÃ­ch lÅ©y kinh nghiá»‡m thá»±c táº¿ vÃ  tá»«ng bÆ°á»›c phÃ¡t triá»ƒn nÄƒng lá»±c chuyÃªn mÃ´n. Má»¥c tiÃªu trong 3-4 nÄƒm tá»›i cá»§a tÃ´i lÃ  trá»Ÿ thÃ nh má»™t AI Engineer cÃ³ ná»n táº£ng vá»¯ng cháº¯c vá» láº­p trÃ¬nh, xá»­ lÃ½ dá»¯ liá»‡u vÃ  triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y.TÃ´i mong muá»‘n Ä‘Æ°á»£c tham gia vÃ o cÃ¡c dá»± Ã¡n AI thá»±c táº¿ Ä‘á»ƒ tÃ­ch lÅ©y kinh nghiá»‡m, tá»«ng bÆ°á»›c hoÃ n thiá»‡n ká»¹ nÄƒng xÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh AI, hÆ°á»›ng tá»›i vai trÃ² chuyÃªn sÃ¢u trong lÄ©nh vá»±c TrÃ­ tuá»‡ nhÃ¢n táº¡o. Há»ŒC Váº¤N TrÆ°á»ng Äáº¡i há»c CÃ´ng Nghiá»‡p Khoa há»c mÃ¡y tÃ­nh 08/2021-06/2025 GPA 3.2 Ká»¸ NÄ‚NG Há»‡ Ä‘iá»u hÃ nh Window, Linux NgÃ´n ngá»¯ láº­p trÃ¬nh Python, C++ Service Pytorch, Tensorow CÃ´ng cá»¥ DevOps Git Cloud Google Colab, Kaggle Notebooks KINH NGHIá»†M LÃ€M VIá»†C CÃ”NG TY TNHH HEXAGON Nam , , Dá»° ÃN - XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2. - XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean. - PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\" qua áº£nh chá»¥p. CHá»¨NG CHá»ˆ TOIEC (735/990) 2024 Dev tool Python 01/2022-03/2025 -Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng . Nhá»¯ng Ä‘iá»u Ä‘áº¡t Ä‘Æ°á»£c: - Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á» -ÄÆ°á»£c tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p. TIáº¾NG ANH Nghe Äá»c Æ¯U ÄIá»‚M TÃ­nh ká»· luáº­t cao: tuÃ¢n thá»§ quy Ä‘á»‹nh khi lÃ m viá»‡c. TÃ¬m kiáº¿m thÃ´ng tin vÃ  giáº£i quyáº¿t váº¥n Ä‘á». ThÃ­ch há»c há»i ThÃ nh tháº¡o sá»­ dá»¥ng cÃ´ng cá»¥ AI: Chatgpt, Gemini, Cursor,... LIÃŠN Káº¾T https://github.com/Huypluie/Huy/tree /develop Â© Joboko.com\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import google.generativeai as genai\nimport json\n\ngenai.configure(api_key=\"AIzaSyDDTl24qBeuxD2tPM7N2pS8iUZfktVTQ7w\")\n\nprompt = f\"\"\"\nPhÃ¢n tÃ­ch ná»™i dung CV sau (Ä‘Ã£ loáº¡i bá» thÃ´ng tin cÃ¡ nhÃ¢n):\n\n--- CV ---\n{clean_text}\n-------------\nCV nÃ y bá»‹ trÃ­ch xuáº¥t tá»« PDF layout phá»©c táº¡p sang nÃªn hÆ¡i bá»‹ láº«n lá»™n text hay Ä‘á»c vÃ  sáº¯p xáº¿p láº¡i sau Ä‘Ã³\nTrÃ­ch xuáº¥t cÃ¡c thÃ´ng tin sau dÆ°á»›i dáº¡ng JSON cÃ³ cáº¥u trÃºc nhÆ° sau:\n{{\n  \"Education\": [\n    {{\n      \"University\": \"...\",\n      \"Faculty\": \"...\",\n      \"CPA\": \"...\",\n      \"Start\": \"...\",\n      \"End\": \"...\"\n    }}\n  ],\n  \"Skills\": [\n    \"...\"\n  ],\n  \"Experience\": [\n    {{\n      \"Company\": \"...\",\n      \"Position\": \"...\",\n      \"Description\": \"...\",\n      \"Start\": \"...\",\n      \"End\": \"...\"\n    }}\n  ],\n    \"Project\": [\n    {{\n      \"Name\": \"...\",\n      \"Year\": \"...\"\n    }}\n  ],\n  \"Certificates\": [\n    {{\n      \"Name\": \"...\",\n      \"Score\":\"...\",\n      \"Year\": \"...\"\n    }}\n  ],\n  \"Languages\": [\n    {{\n      \"Language\": \"...\",\n      \"Skills\": [\"Nghe\", \"NÃ³i\", \"Äá»c\", \"Viáº¿t\"],\n      \"Certificate\": {{\n        \"Name\": \"...\",\n        \"Score\": \"...\"\n      }}\n    }}\n  ]\n}}\n\nâš ï¸ Ghi chÃº:\n- Giá»¯ nguyÃªn thá»© tiáº¿ng trong CV khÃ´ng tá»± chuyá»ƒn sang tiáº¿ng Anh\n- CÃ¡c má»¥c Education vÃ  Experience sáº¯p xáº¿p theo thá»i gian (gáº§n nháº¥t trÆ°á»›c).\n- Náº¿u khÃ´ng cÃ³ thÃ´ng tin, Ä‘á»ƒ trá»‘ng chuá»—i (\"\").\n- Ká»¹ nÄƒng nÃªn bao gá»“m cáº£ ká»¹ nÄƒng suy ra tá»« kinh nghiá»‡m vÃ  dá»± Ã¡n.\n- Experience náº¿u chá»‰ tÃ¬m tháº¥y 1 tÃªn cÃ´ng ty thÃ¬ cÃ³ nghÄ©a chá»‰ cÃ³ 1 kinh nghiá»‡m Ä‘Ã³, chá»‰ rÃµ tÃªn cÃ´ng ty\n\"\"\"\n\nresponse = genai.GenerativeModel(\"gemini-2.5-pro\").generate_content(prompt)\n\n# âœ… TrÃ­ch riÃªng pháº§n JSON trong káº¿t quáº£\nmatch = re.search(r\"\\{[\\s\\S]*\\}\", response.text)\nif match:\n    extracted_json = match.group(0)\n    try:\n        data = json.loads(extracted_json)\n        print(json.dumps(data, ensure_ascii=False, indent=4))\n    except:\n        print(\"âš ï¸ JSON khÃ´ng há»£p lá»‡:\\n\", extracted_json)\nelse:\n    print(\"âŒ KhÃ´ng tÃ¬m tháº¥y JSON trong pháº£n há»“i.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:08:46.716399Z","iopub.execute_input":"2025-10-21T14:08:46.716717Z","iopub.status.idle":"2025-10-21T14:11:22.497311Z","shell.execute_reply.started":"2025-10-21T14:08:46.716695Z","shell.execute_reply":"2025-10-21T14:11:22.496613Z"}},"outputs":[{"name":"stdout","text":"{\n    \"Education\": [\n        {\n            \"University\": \"TrÆ°á»ng Äáº¡i há»c CÃ´ng Nghiá»‡p\",\n            \"Faculty\": \"Khoa há»c mÃ¡y tÃ­nh\",\n            \"GPA\": \"3.2\",\n            \"Start\": \"08/2021\",\n            \"End\": \"06/2025\"\n        }\n    ],\n    \"Skills\": [\n        \"Window\",\n        \"Linux\",\n        \"Python\",\n        \"C++\",\n        \"Pytorch\",\n        \"Tensorow\",\n        \"Git\",\n        \"Google Colab\",\n        \"Kaggle Notebooks\",\n        \"Chatgpt\",\n        \"Gemini\",\n        \"Cursor\",\n        \"TÆ° duy ká»¹ thuáº­t\",\n        \"TÆ° duy thuáº­t toÃ¡n\",\n        \"Há»c há»i cÃ´ng nghá»‡ má»›i nhanh\",\n        \"Láº­p trÃ¬nh\",\n        \"Xá»­ lÃ½ dá»¯ liá»‡u\",\n        \"Triá»ƒn khai mÃ´ hÃ¬nh há»c mÃ¡y\",\n        \"XÃ¢y dá»±ng mÃ´ hÃ¬nh AI\",\n        \"Huáº¥n luyá»‡n mÃ´ hÃ¬nh AI\",\n        \"Giáº£i quyáº¿t váº¥n Ä‘á»\",\n        \"LÃ m viá»‡c nhÃ³m\",\n        \"Tá»± há»c\",\n        \"TÃ¬m kiáº¿m thÃ´ng tin\",\n        \"TÃ­nh ká»· luáº­t cao\",\n        \"Tá»± Ä‘á»™ng hÃ³a\",\n        \"NLP\",\n        \"GPT-2\",\n        \"K-mean\",\n        \"Computer Vision\",\n        \"Machine Learning\"\n    ],\n    \"Experience\": [\n        {\n            \"Company\": \"CÃ”NG TY TNHH HEXAGON\",\n            \"Position\": \"Dev tool Python\",\n            \"Description\": \"Viáº¿t tool há»— trá»£ tá»± Ä‘á»™ng hÃ³a cÃ¡c chá»©c nÄƒng. Há»c há»i Ä‘Æ°á»£c kÄ© nÄƒng láº­p trÃ¬nh, lÃ m viá»‡c nhÃ³m vÃ  tá»± giáº£i quyáº¿t váº¥n Ä‘á». Tiáº¿p cáº­n há»c há»i cÃ¡c cÃ´ng nghá»‡ má»›i, cÃ¡c bÃ i toÃ¡n nghiá»‡p vá»¥ phá»©c táº¡p.\",\n            \"Start\": \"01/2022\",\n            \"End\": \"03/2025\"\n        }\n    ],\n    \"Project\": [\n        {\n            \"Name\": \"XÃ¢y dá»±ng mÃ´ hÃ¬nh há»— trá»£ hoÃ n thiá»‡n cÃ¢u vá»›i GPT-2\",\n            \"Year\": \"\"\n        },\n        {\n            \"Name\": \"XÃ¢y dá»±ng há»‡ thá»‘ng cáº£nh bÃ¡o ngá»§ gáº­t trÃªn xe vá»›i Kmean\",\n            \"Year\": \"\"\n        },\n        {\n            \"Name\": \"PhÃ¡t triá»ƒn há»‡ thá»‘ng há»— trá»£ dá»± Ä‘oÃ¡n bá»‡nh \\\"Tiá»ƒu Ä‘Æ°á»ng giÃ¡c máº¡c\\\" qua áº£nh chá»¥p\",\n            \"Year\": \"\"\n        }\n    ],\n    \"Certificates\": [\n        {\n            \"Name\": \"TOIEC\",\n            \"Score\": \"735/990\",\n            \"Year\": \"2024\"\n        }\n    ],\n    \"Languages\": [\n        {\n            \"Language\": \"Tiáº¿ng Anh\",\n            \"Skills\": [\n                \"Nghe\",\n                \"Äá»c\"\n            ],\n            \"Certificate\": {\n                \"Name\": \"TOIEC\",\n                \"Score\": \"735/990\"\n            }\n        }\n    ]\n}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import os\nfrom google import genai\nfrom google.genai import types\n\n# 1. Khá»Ÿi táº¡o Client\nclient = genai.Client(api_key=\"AIzaSyDDTl24qBeuxD2tPM7N2pS8iUZfktVTQ7w\")\n\n# 2. Äá»c tá»‡p PDF dÆ°á»›i dáº¡ng byte\npdf_path = '/kaggle/input/cv-huy/nguyen-duc-huy_1758517966_Joboko_c3e1a50bcfd6fb7f_3487225.pdf'\nwith open(pdf_path, 'rb') as f:\n    pdf_bytes = f.read()\n\n# 3. Táº¡o Content Part (Mime Type: application/pdf)\npdf_part = types.Part.from_bytes(\n    data=pdf_bytes,\n    mime_type='application/pdf'\n)\njd_text = \"\"\"MÃ´ táº£ cÃ´ng viá»‡c\nâ— PhÃ¡t triá»ƒn vÃ  tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯: Sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ vÃ  framework nhÆ° TensorFlow, PyTorch, vÃ  Hugging Face Transformers Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯.\n\nâ— PhÃ¢n tÃ­ch vÃ  xá»­ lÃ½ dá»¯ liá»‡u ngÃ´n ngá»¯: Sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t NLP Ä‘á»ƒ phÃ¢n tÃ­ch, trÃ­ch xuáº¥t thÃ´ng tin tá»« vÄƒn báº£n, vÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.\n\nâ— Thiáº¿t káº¿ há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin: PhÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ há»— trá»£ quÃ¡ trÃ¬nh táº¡o ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§.\n\nâ— Káº¿t há»£p truy xuáº¥t vÃ  sinh vÄƒn báº£n: Sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t RAG Ä‘á»ƒ káº¿t há»£p thÃ´ng tin truy xuáº¥t tá»« cÃ¡c nguá»“n dá»¯ liá»‡u vá»›i kháº£ nÄƒng sinh vÄƒn báº£n cá»§a mÃ´ hÃ¬nh.\n\nâ— NghiÃªn cá»©u cÃ¡c ká»¹ thuáº­t má»›i: Theo dÃµi vÃ  nghiÃªn cá»©u cÃ¡c xu hÆ°á»›ng vÃ  cÃ´ng nghá»‡ má»›i trong lÄ©nh vá»±c NLP, Chatbot vÃ  RAG.\n\nâ— Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t há»‡ thá»‘ng: Tá»‘i Æ°u hÃ³a thá»i gian pháº£n há»“i vÃ  hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin.\n\nYÃªu cáº§u á»©ng viÃªn\nâ— Kinh nghiá»‡m: Tá»‘i thiá»ƒu 1 nÄƒm á»Ÿ vá»‹ trÃ­ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n\nâ— TrÃ¬nh Ä‘á»™ há»c váº¥n: Tá»‘t nghiá»‡p Cao Ä‘áº³ng/Äáº¡i há»c cÃ¡c chuyÃªn ngÃ nh CÃ´ng nghá»‡ ThÃ´ng tin, ToÃ¡n Tin, Äiá»‡n tá»­ Viá»…n thÃ´ng, Äiá»u khiá»ƒn Tá»± Ä‘á»™ng, hoáº·c cÃ¡c ngÃ nh liÃªn quan.\n\nâ— Kiáº¿n thá»©c chuyÃªn mÃ´n:\n\n- CÃ³ hiá»ƒu biáº¿t vá» Machine Learning vÃ  Deep Learning.\n\n- Kinh nghiá»‡m lÃ m viá»‡c vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° BERT, T5, Mistral, LLaMa, GPT, v.v.\n\n- CÃ³ kinh nghiá»‡m lÃ m viá»‡c vá»›i RESTAPI, Langchain, llamaindex, ...\n\nâ— Ká»¹ nÄƒng nghiÃªn cá»©u vÃ  ná»n táº£ng:\n\n- Kháº£ nÄƒng nghiÃªn cá»©u vÃ  Ã¡p dá»¥ng cÃ¡c cÃ´ng nghá»‡ má»›i.\n\n- Ná»n táº£ng vá»¯ng cháº¯c vá» cáº¥u trÃºc dá»¯ liá»‡u vÃ  thuáº­t toÃ¡n.\n\nQuyá»n lá»£i\nâ— Má»©c lÆ°Æ¡ng: tá»« 13 - 18M/thÃ¡ng (thá»a thuáº­n khi phá»ng váº¥n)\n\nâ— CÃ´ng ty Ä‘Ã³ng BHYT, BHXH, BHTN theo quy Ä‘á»‹nh\n\nâ— CÃ´ng ty cung cáº¥p thiáº¿t bá»‹ lÃ m viá»‡c\n\nâ— Review lÆ°Æ¡ng 1-2 láº§n/nÄƒm theo nÄƒng lá»±c\n\nâ— ThÆ°á»Ÿng ngÃ y lá»… 2/9, 30/04, 1/5, ... thÆ°á»Ÿng lÆ°Æ¡ng thÃ¡ng 13\n\nâ— ThÆ°á»Ÿng káº¿t quáº£ kinh doanh toÃ n cÃ´ng ty cuá»‘i nÄƒm\n\nâ— Du lá»‹ch, nghá»‰ mÃ¡t hÃ ng nÄƒm\n\nâ— MÃ´i trÆ°á»ng lÃ m viá»‡c nÄƒng Ä‘á»™ng, chuyÃªn nghiá»‡p, táº¡o cÆ¡ há»™i cho nhÃ¢n viÃªn thá»a sá»©c sÃ¡ng táº¡o vÃ  phÃ¡t triá»ƒn báº£n thÃ¢n\n\nâ— Pantry: Coffee, MÃ¡y pha coffee, Tá»§ láº¡nh\n\nÄá»‹a Ä‘iá»ƒm lÃ m viá»‡c\n- HÃ  Ná»™i: Khu VP táº§ng 3, tÃ²a nhÃ  CT1 Constrexim ThÃ¡i HÃ , Pháº¡m VÄƒn Äá»“ng, Cá»• Nhuáº¿ 2, Báº¯c Tá»« LiÃªm\n\"\"\"\nprompt = \"\"\"\n     lá»c ra tá»« CV, hÃ£y Ä‘á»c vÃ  TrÃ­ch xuáº¥t thÃ´ng tin chuáº©n chá»‰ trong vÄƒn báº£n:\n        - \"skills\": danh sÃ¡ch ká»¹ nÄƒng(dáº¡ng list)\n       - \"experience_years\": sá»‘ nÄƒm kinh nghiá»‡m (sá»‘ nguyÃªn)\n        - \"education\": báº±ng cáº¥p cao nháº¥t (chuá»—i)\n        - \"certificates\": danh sÃ¡ch chá»©ng chá»‰ cÃ³ (dáº¡ng list)\n    Sau Ä‘Ã³ tÃ­nh matching score vá»›i JD theo tá»«ng cÃ¡c tiÃªu chÃ­ skills, experience_years, education, certificates má»—i cÃ¡i gá»“m 3 má»¥c:\n        -\"score\": Ä‘iá»ƒm cá»§a tiÃªu chÃ­ Ä‘Ã³\n        -\"missing\": cÃ¡c thá»© cÃ²n thiáº¿u thiáº¿u\n        -\"match\": cac thá»© Ä‘Ã¡p á»©ng Ä‘Æ°á»£c (náº¿u vÆ°á»£t qua yÃªu cáº§u JD thÃ¬ cá»™ng thÃªm chÃºt Ä‘iá»ƒm)\n    Cuá»‘i cÃ¹ng Ä‘Æ°a ra Ä‘iá»ƒm matching tá»•ng tháº¿, lÆ°u Ã½ tÃ¹y theo JD yÃªu cáº§u Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i tiÃªu chÃ­ nÃ o thÃ¬ cho tiÃªu chÃ­ Ä‘Ã³ hiá»‡u sá»‘ cao lÃªn trong tÃ­nh Ä‘iá»ƒm matching cuá»‘i\n    HÃ£y tráº£ lá»i **CHá»ˆ** báº±ng má»™t Ä‘á»‘i tÆ°á»£ng JSON, **KHÃ”NG** thÃªm báº¥t ká»³ lá»i giáº£i thÃ­ch hay vÄƒn báº£n nÃ o khÃ¡c.\n\"\"\"\n# 4. Gá»i API vá»›i PDF vÃ  Text Prompt\nresponse = client.models.generate_content(\n    model='gemini-2.5-pro',\n    contents=[\n        pdf_part, jd_text,prompt\n        \n         # VÄƒn báº£n prompt\n    ]\n)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:23:54.817897Z","iopub.execute_input":"2025-10-21T14:23:54.818180Z","iopub.status.idle":"2025-10-21T14:24:21.670025Z","shell.execute_reply.started":"2025-10-21T14:23:54.818158Z","shell.execute_reply":"2025-10-21T14:24:21.669337Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"extracted_info\": {\n    \"skills\": [\n      \"Window\",\n      \"Linux\",\n      \"Python\",\n      \"C++\",\n      \"Pytorch\",\n      \"Tensorflow\",\n      \"Git\",\n      \"Google Colab\",\n      \"Kaggle Notebooks\",\n      \"GPT-2\",\n      \"K-means\"\n    ],\n    \"experience_years\": 2,\n    \"education\": \"Äáº¡i há»c\",\n    \"certificates\": [\n      \"TOEIC (735/990)\"\n    ]\n  },\n  \"matching_summary\": {\n    \"skills\": {\n      \"score\": 6.5,\n      \"missing\": [\n        \"RESTAPI\",\n        \"Langchain\",\n        \"llamaindex\",\n        \"Kinh nghiá»‡m vá»›i cÃ¡c LLM khÃ¡c (BERT, T5, Mistral, LLaMa)\"\n      ],\n      \"match\": [\n        \"Hiá»ƒu biáº¿t vá» Machine Learning vÃ  Deep Learning\",\n        \"Kinh nghiá»‡m lÃ m viá»‡c vá»›i LLM (GPT-2)\",\n        \"Pytorch\",\n        \"Tensorflow\",\n        \"Ná»n táº£ng vá» cáº¥u trÃºc dá»¯ liá»‡u vÃ  thuáº­t toÃ¡n (do há»c ngÃ nh CNTT)\"\n      ]\n    },\n    \"experience_years\": {\n      \"score\": 10,\n      \"missing\": [],\n      \"match\": [\n        \"CÃ³ khoáº£ng 2.5 nÄƒm kinh nghiá»‡m, vÆ°á»£t yÃªu cáº§u tá»‘i thiá»ƒu 1 nÄƒm cá»§a JD\"\n      ]\n    },\n    \"education\": {\n      \"score\": 9,\n      \"missing\": [\n        \"ChÆ°a chÃ­nh thá»©c tá»‘t nghiá»‡p (dá»± kiáº¿n 06/2025)\"\n      ],\n      \"match\": [\n        \"Há»c Äáº¡i há»c chuyÃªn ngÃ nh Khoa há»c mÃ¡y tÃ­nh, phÃ¹ há»£p hoÃ n toÃ n vá»›i yÃªu cáº§u vá» ngÃ nh há»c\"\n      ]\n    },\n    \"certificates\": {\n      \"score\": 7,\n      \"missing\": [\n        \"KhÃ´ng cÃ³ chá»©ng chá»‰ chuyÃªn mÃ´n ká»¹ thuáº­t Ä‘Æ°á»£c liá»‡t kÃª\"\n      ],\n      \"match\": [\n        \"CÃ³ chá»©ng chá»‰ TOEIC, lÃ  má»™t Ä‘iá»ƒm cá»™ng vá» kháº£ nÄƒng ngoáº¡i ngá»¯\"\n      ]\n    },\n    \"overall_score\": 7.85\n  }\n}\n```\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}